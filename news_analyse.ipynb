{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".container { width:100% }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".container { width:100% }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import der benötigten Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                            # bilden der Bigrams\n",
    "import operator                        # sortieren des Dictionarys\n",
    "import re                              # Regular Expressions, Entfernen von Zeichen\n",
    "import pandas    as pd                 # .txt konvertieren, .csv einlesen und abspeichern\n",
    "from pymongo     import MongoClient    # Verbindung und Interaktion mit mongoDB\n",
    "from string      import ascii_letters  # Ascii-Buchstaben, die erhalten bleiben\n",
    "from pathlib     import Path           # Prüfen, ob Datei vorhanden ist\n",
    "from nltk.corpus import stopwords      # Füllworte\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden der Sätze und falls notwendig Konvertierung der .txt-Datenquellen in .csv-Dateien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path('sentences_de.csv').is_file():\n",
    "    sentences_csv = pd.read_csv ('sentences_de.csv', names=['id', 'sentence'], header = 0)\n",
    "elif Path('deu_news_2015_3M-sentences.txt').is_file():\n",
    "    sentences_csv = pd.read_csv ('deu_news_2015_3M-sentences.txt', names=['id', 'sentence'], header = 0, delimiter='\\t')\n",
    "    sentences_csv.to_csv ('sentences_de.csv', index=None)\n",
    "else:\n",
    "    print(\"Die benötigten Daten existieren nicht. Bitte lade sie von folgender Seite runter: https://www.kaggle.com/rtatman/3-million-german-sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden der Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Luisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('german'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entfernen der Stopwords und allen Zeichen, die keine Ascii-Buchstaben sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_and_nonascii(sentences):\n",
    "    queue = []\n",
    "    for sentence in sentences:\n",
    "        just_ascii = re.sub(r\"[^{}]\".format(ascii_letters + 'äöüÄÖÜß'), ' ', sentence) \n",
    "        result = ' '.join([i for i in just_ascii.lower().split() if i not in stop and i != ' '])\n",
    "        if result != ' ': queue.append(result)\n",
    "    return queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nostop = remove_stopwords_and_nonascii(sentences_csv['sentence'])\n",
    "nostop = pd.DataFrame(nostop, columns=['sentence'])\n",
    "nostop.to_csv('sentences_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verbindung mit mongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string      = \"mongodb://localhost:27017\"\n",
    "db                     = MongoClient(connection_string)\n",
    "sentences              = db.dw.sentences\n",
    "sentences_preprocessed = db.dw.sentences_preprocessed\n",
    "bigrams                = db.dw.bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences und Stopwords der Database in mongoDB hinzufügen\n",
    "\n",
    "`save_to_mongo()` speichert Listen von Dictionarys oder DataFrames in einer Collection. Dafür kann zunächst die Collection gelöscht werden (`drop_collection = True`). Sonst werden die Daten zusätzlich in die Collection eingefügt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_mongo(collection, data, drop_collection):\n",
    "    if drop_collection:\n",
    "        collection.drop()\n",
    "    if isinstance(data, pd.DataFrame): data = data.to_dict('records')\n",
    "    result = collection.insert_many(data)\n",
    "    print('%d rows are saved to \"%s\" collection successfully!' % (len(result.inserted_ids), collection.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2646182 rows are saved to \"sentences\" collection successfully!\n"
     ]
    }
   ],
   "source": [
    "save_to_mongo(sentences, sentences_csv, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2646182 rows are saved to \"sentences_preprocessed\" collection successfully!\n"
     ]
    }
   ],
   "source": [
    "save_to_mongo(sentences_preprocessed, nostop, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraktion der Daten\n",
    "`get_column_of_collection()` liest alle Datensätze aus einer Spalte einer Collection und gibt eine Liste mit diesen Datensätzen zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_of_collection(collection, column):\n",
    "    documents = []\n",
    "    cursor = collection.find({})\n",
    "    for document in cursor:\n",
    "        documents.append(document[column])\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_documents = get_column_of_collection(sentences_preprocessed, 'sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finden der häufisten Bigrams (= 2-Wort-Kombinationen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`count_bigrams()` nimmt eine Liste von Sätzen, bildet alle Bigrams, und speichert diese mit ihrer Häufigkeit in einem Dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_bigrams(data):\n",
    "    bigram_dict = dict()\n",
    "    for i in range(len(data)):\n",
    "        text = data[i]\n",
    "        bigrams = list(nltk.bigrams(text.split()))\n",
    "        for bigram in bigrams:\n",
    "            if bigram in bigram_dict:\n",
    "                bigram_dict[bigram] += 1\n",
    "            else:\n",
    "                bigram_dict[bigram] = 1\n",
    "    return bigram_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_k_most_frequent()` nimmt ein zuvor mit `count_bigrams()`erstelltes Dictionary, sortiert es nach den Haüfigkeiten, und gibt die k Elemente mit der größten Häufigkeit zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_most_frequent(dictionary, k):\n",
    "    sorted_dictionary =  dict(sorted(dictionary.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    result = sorted_dictionary.items()\n",
    "    return list(result)[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pretty_print_most_frequent()` nimmt die von `get_k_most_frequent()` ausgegebenen Datensätze und gibt sie schön formatiert aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_most_frequent(data):\n",
    "    for i in range(len(data)):\n",
    "        words, frequency = data[i]\n",
    "        print('Platz ', i+1, ': Die Wortkombination \"', ' '.join(words), \\\n",
    "              '\" kommt in den Daten ', frequency, ' mal vor.', sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bigrams_to_dict()` bereitet die Bigrams so vor, dass sie in einer Collection gespeichert werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams_to_dict(bigrams):\n",
    "    result = []\n",
    "    for item in bigrams:\n",
    "        dictionary = {'bigram': item, 'frequency': bigrams[item]}\n",
    "        result.append(dictionary)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_k_most_frequent_bigrams()` ruft die zuvor implementierten Funktionen auf. Zunächst werden die Bigrams gebildet und gezählt. Diese werden dann formatiert und in der Datenbank gespeichert. Anschließend werden die k häufigsten Bigrams schön ausgedruckt und schließlich auch zurückgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_most_frequent_bigrams(data, k, collection_to_save):\n",
    "    bigram_dict   = count_bigrams(data)\n",
    "    saveable_dict = bigrams_to_dict(bigram_dict)\n",
    "    save_to_mongo(collection_to_save, saveable_dict, True)\n",
    "    most_frequent = get_k_most_frequent(bigram_dict, k)\n",
    "    pretty_print_most_frequent(most_frequent)\n",
    "    return most_frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13480027 rows are saved to \"bigrams\" collection successfully!\n",
      "Platz 1: Die Wortkombination \"e mail\" kommt in den Daten 15108 mal vor.\n",
      "Platz 2: Die Wortkombination \"millionen euro\" kommt in den Daten 11995 mal vor.\n",
      "Platz 3: Die Wortkombination \"milliarden euro\" kommt in den Daten 8993 mal vor.\n",
      "Platz 4: Die Wortkombination \"antwort schreiben\" kommt in den Daten 7553 mal vor.\n",
      "Platz 5: Die Wortkombination \"seit jahren\" kommt in den Daten 6488 mal vor.\n",
      "Platz 6: Die Wortkombination \"new york\" kommt in den Daten 5108 mal vor.\n",
      "Platz 7: Die Wortkombination \"us dollar\" kommt in den Daten 4957 mal vor.\n",
      "Platz 8: Die Wortkombination \"mail adresse\" kommt in den Daten 4883 mal vor.\n",
      "Platz 9: Die Wortkombination \"orf at\" kommt in den Daten 4797 mal vor.\n",
      "Platz 10: Die Wortkombination \"vergangenen jahr\" kommt in den Daten 4776 mal vor.\n",
      "Wall time: 57min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "most_frequent = get_k_most_frequent_bigrams(extracted_documents, 10, bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDos\n",
    "- Einlesen von csv ohne Konvertierung ✓\n",
    "- Einfügen von sentences_preprocessed in DB ohne Umweg über DataFrame und .csv ✓\n",
    "- Spaltenname ohne Umbenennung ✓\n",
    "- Daten für Analyse aus DB ziehen ✓\n",
    "- Das Ergebnis (komplettes Bigram-Dict?) in der DB speichern ✓\n",
    "- Weitere Analysen?\n",
    "- Satzzeichen bei Stopwörtern hizufügen ✓"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
