{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".container { width:100% }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".container { width:100% }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import der benötigten Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import nltk\n",
    "import csv\n",
    "import operator\n",
    "import re\n",
    "import pandas    as pd\n",
    "from pymongo     import MongoClient\n",
    "from nltk        import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string      import ascii_letters\n",
    "from pathlib     import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden der Sätze und falls notwendig konvertierung der .txt-Datenquellen in .csv-Dateien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path('sentences_de.csv').is_file():\n",
    "    sentences_csv = pd.read_csv ('sentences_de.csv', header = None)\n",
    "elif Path('deu_news_2015_3M-sentences.txt').is_file():\n",
    "    sentences_csv = pd.read_csv ('deu_news_2015_3M-sentences.txt', header = None, delimiter='\\t')\n",
    "    sentences_csv.to_csv ('sentences_de.csv', index=None)\n",
    "else:\n",
    "    print(\"Die benötigten Daten existieren nicht. Bitte lade sie von folgender Seite runter: https://www.kaggle.com/rtatman/3-million-german-sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden der Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Luisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('german'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entfernen der Stopwords und allen Zeichen, die keine Ascii-Buchstaben sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_and_nonascii(sentences):\n",
    "    queue = []\n",
    "    for sentence in sentences:\n",
    "        just_ascii = re.sub(r\"[^{}]\".format(ascii_letters + 'äöüÄÖÜß'), ' ', sentence) \n",
    "        result = ' '.join([i for i in just_ascii.lower().split() if i not in stop and i != ' '])\n",
    "        if result != ' ': queue.append(result)\n",
    "    return queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nostop = remove_stopwords_and_nonascii(sentences_csv[1]) #(sentences_csv[1][100:200]) Für Test nur 100 Datensätze verwenden\n",
    "df = pd.DataFrame(nostop)\n",
    "df.to_csv('sentences_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verbindung mit mongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string      = \"mongodb://localhost:27017\"\n",
    "db                     = MongoClient(connection_string)\n",
    "sentences              = db.dw.sentences\n",
    "sentences_preprocessed = db.dw.sentences_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences und Stopwords der Database in mongoDB hinzufügen\n",
    "\n",
    "`read_csv()` liest eine .csv-Datei ein und gibt ein Dictionary zurück, dass dann mit `save_to_mongo()` in einer Collection gespeichert wird. Dafür kann zunächst die Collection gelöscht werden (`drop_collection = True`). Sonst werden die Daten zusätzlich in die Collection eingefügt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(path, columns):\n",
    "    reader = open(path, 'r', encoding='utf-8')\n",
    "    return csv.DictReader(reader, columns)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_mongo(collection, path, columns, drop_collection):\n",
    "    if drop_collection:\n",
    "        collection.drop()\n",
    "    data = read_csv(path, columns)\n",
    "    result = collection.insert_many(data)\n",
    "    print('%d rows are saved to \"%s\" collection successfully!' % (len(result.inserted_ids), collection.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2646184 rows are saved to \"sentences\" collection successfully!\n"
     ]
    }
   ],
   "source": [
    "save_to_mongo(sentences, 'sentences_de.csv', ('id','sentence'), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2646185 rows are saved to \"sentences_preprocessed\" collection successfully!\n"
     ]
    }
   ],
   "source": [
    "save_to_mongo(sentences_preprocessed, 'sentences_preprocessed.csv', ('s'), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn ich die als Column-Name gleich `sentence` genommen habe, dann hat er daraus eine Spalte `s`, eine `e`, eine `n` und so weiter gemacht...Deswegen hier mit der späteren Umbenennung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x2d9d8e9ac00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_preprocessed.update_many( {}, { '$rename': { \"s\": \"sentence\" } } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_bigrams(data):\n",
    "    bigram_dict = dict()\n",
    "    for text in data:\n",
    "        bigrams = list(nltk.bigrams(text.split()))\n",
    "        for bigram in bigrams:\n",
    "            if bigram in bigram_dict:\n",
    "                bigram_dict[bigram] += 1\n",
    "            else:\n",
    "                bigram_dict[bigram] = 1\n",
    "    return bigram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_most_frequent(dictionary, k):\n",
    "    sorted_dictionary =  dict(sorted(dictionary.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    result = sorted_dictionary.items()\n",
    "    return list(result)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_most_frequent(data):\n",
    "    for i in range(len(data)):\n",
    "        words, frequency = data[i]\n",
    "        print('Platz ', i+1, ': Die Wortkombination \"', ' '.join(words), \\\n",
    "              '\" kommt in den Daten ', frequency, ' mal vor.', sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_most_frequent_bigrams(data, k):\n",
    "    bigram_dict   = count_bigrams(data)\n",
    "    most_frequent = get_k_most_frequent(bigram_dict, k)\n",
    "    pretty_print_most_frequent(most_frequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platz 1: Die Wortkombination \"e mail\" kommt in den Daten 15108 mal vor.\n",
      "Platz 2: Die Wortkombination \"millionen euro\" kommt in den Daten 11995 mal vor.\n",
      "Platz 3: Die Wortkombination \"milliarden euro\" kommt in den Daten 8993 mal vor.\n",
      "Platz 4: Die Wortkombination \"antwort schreiben\" kommt in den Daten 7553 mal vor.\n",
      "Platz 5: Die Wortkombination \"seit jahren\" kommt in den Daten 6488 mal vor.\n",
      "Platz 6: Die Wortkombination \"new york\" kommt in den Daten 5108 mal vor.\n",
      "Platz 7: Die Wortkombination \"us dollar\" kommt in den Daten 4957 mal vor.\n",
      "Platz 8: Die Wortkombination \"mail adresse\" kommt in den Daten 4883 mal vor.\n",
      "Platz 9: Die Wortkombination \"orf at\" kommt in den Daten 4797 mal vor.\n",
      "Platz 10: Die Wortkombination \"vergangenen jahr\" kommt in den Daten 4776 mal vor.\n"
     ]
    }
   ],
   "source": [
    "get_k_most_frequent_bigrams(nostop, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDos: \n",
    "- Einfügen von sentences_preprocessed in DB ohne Umweg über DataFrame und .csv\n",
    "- Spaltenname ohne Umbenennung\n",
    "- Daten für Analyse aus DB ziehen\n",
    "- Das Ergebnis (komplettes Bigram-Dict?) in der DB speichern\n",
    "- Weitere Analysen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
