{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".container { width:100% }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".container { width:100% }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import der benötigten Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import nltk\n",
    "import csv\n",
    "import operator\n",
    "import re\n",
    "import pandas    as pd\n",
    "from pymongo     import MongoClient\n",
    "from nltk        import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string      import ascii_letters\n",
    "from pathlib     import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden der Sätze und falls notwendig konvertierung der .txt-Datenquellen in .csv-Dateien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path('sentences_de.csv').is_file():\n",
    "    sentences_csv = pd.read_csv ('sentences_de.csv', names=['id', 'sentence'], header = 0)\n",
    "elif Path('deu_news_2015_3M-sentences.txt').is_file():\n",
    "    sentences_csv = pd.read_csv ('deu_news_2015_3M-sentences.txt', names=['id', 'sentence'], header = 0, delimiter='\\t')\n",
    "    sentences_csv.to_csv ('sentences_de.csv', index=None)\n",
    "else:\n",
    "    print(\"Die benötigten Daten existieren nicht. Bitte lade sie von folgender Seite runter: https://www.kaggle.com/rtatman/3-million-german-sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden der Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Luisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('german'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entfernen der Stopwords und allen Zeichen, die keine Ascii-Buchstaben sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_and_nonascii(sentences):\n",
    "    queue = []\n",
    "    for sentence in sentences:\n",
    "        just_ascii = re.sub(r\"[^{}]\".format(ascii_letters + 'äöüÄÖÜß'), ' ', sentence) \n",
    "        result = ' '.join([i for i in just_ascii.lower().split() if i not in stop and i != ' '])\n",
    "        if result != ' ': queue.append(result)\n",
    "    return queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0,00001% der Stimmrechte (das entspricht 14 St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0,00001% der Stimmrechte (das entspricht 257 S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>00:00 DGAP-AFR: AGROB Immobilien AG: Bekanntma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>00.00 - Schalten Sie das iPad ab: Eine Stunde ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2646177</th>\n",
       "      <td>2791242</td>\n",
       "      <td>ريف اللاذقية – القصاص من معركة القصاص 5 10 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2646178</th>\n",
       "      <td>2791243</td>\n",
       "      <td>Мeinungen“ befragt wurden, meinen, dass die Sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2646179</th>\n",
       "      <td>2791244</td>\n",
       "      <td>Реалии (@krymrealii) January 19, 2015 Die Behö...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2646180</th>\n",
       "      <td>2791245</td>\n",
       "      <td>Ру (@GraniTweet) 1. März 2015 Der Mord sorgt i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2646181</th>\n",
       "      <td>2791246</td>\n",
       "      <td>Тренировка бойцов «Азова» озадачила пользовате...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2646182 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           sentence\n",
       "0              2                                                  .\n",
       "1              3  0,00001% der Stimmrechte (das entspricht 14 St...\n",
       "2              4  0,00001% der Stimmrechte (das entspricht 257 S...\n",
       "3              5  00:00 DGAP-AFR: AGROB Immobilien AG: Bekanntma...\n",
       "4              6  00.00 - Schalten Sie das iPad ab: Eine Stunde ...\n",
       "...          ...                                                ...\n",
       "2646177  2791242  ريف اللاذقية – القصاص من معركة القصاص 5 10 201...\n",
       "2646178  2791243  Мeinungen“ befragt wurden, meinen, dass die Sc...\n",
       "2646179  2791244  Реалии (@krymrealii) January 19, 2015 Die Behö...\n",
       "2646180  2791245  Ру (@GraniTweet) 1. März 2015 Der Mord sorgt i...\n",
       "2646181  2791246  Тренировка бойцов «Азова» озадачила пользовате...\n",
       "\n",
       "[2646182 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nostop = remove_stopwords_and_nonascii(sentences_csv['sentence'])\n",
    "nostop = pd.DataFrame(nostop, columns=['sentence'])\n",
    "nostop.to_csv('sentences_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verbindung mit mongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string      = \"mongodb://localhost:27017\"\n",
    "db                     = MongoClient(connection_string)\n",
    "sentences              = db.dw.sentences\n",
    "sentences_preprocessed = db.dw.sentences_preprocessed\n",
    "bigrams                = db.dw.bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences und Stopwords der Database in mongoDB hinzufügen\n",
    "\n",
    "`read_csv()` liest eine .csv-Datei ein und gibt ein Dictionary zurück, dass dann mit `save_to_mongo()` in einer Collection gespeichert wird. Dafür kann zunächst die Collection gelöscht werden (`drop_collection = True`). Sonst werden die Daten zusätzlich in die Collection eingefügt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_mongo(collection, data, drop_collection):\n",
    "    if drop_collection:\n",
    "        collection.drop()\n",
    "    if isinstance(data, pd.DataFrame): data = data.to_dict('records')\n",
    "    result = collection.insert_many(data)\n",
    "    print('%d rows are saved to \"%s\" collection successfully!' % (len(result.inserted_ids), collection.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2646182 rows are saved to \"sentences\" collection successfully!\n"
     ]
    }
   ],
   "source": [
    "save_to_mongo(sentences, sentences_csv, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2646182 rows are saved to \"sentences_preprocessed\" collection successfully!\n"
     ]
    }
   ],
   "source": [
    "save_to_mongo(sentences_preprocessed, nostop, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_of_collection(collection, column):\n",
    "    documents = []\n",
    "    cursor = collection.find({})\n",
    "    for document in cursor:\n",
    "        documents.append(document[column])\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_bigrams(data):\n",
    "    bigram_dict = dict()\n",
    "    for i in range(len(data)):\n",
    "        #text = str(data.iloc[[i],[0]])\n",
    "        text = data[i]\n",
    "        bigrams = list(nltk.bigrams(text.split()))\n",
    "        for bigram in bigrams:\n",
    "            if bigram in bigram_dict:\n",
    "                bigram_dict[bigram] += 1\n",
    "            else:\n",
    "                bigram_dict[bigram] = 1\n",
    "    return bigram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_most_frequent(dictionary, k):\n",
    "    sorted_dictionary =  dict(sorted(dictionary.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    result = sorted_dictionary.items()\n",
    "    return list(result)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_most_frequent(data):\n",
    "    for i in range(len(data)):\n",
    "        words, frequency = data[i]\n",
    "        print('Platz ', i+1, ': Die Wortkombination \"', ' '.join(words), \\\n",
    "              '\" kommt in den Daten ', frequency, ' mal vor.', sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams_to_dict(bigrams):\n",
    "    result = []\n",
    "    for item in bigrams:\n",
    "        dictionary = {'bigram': item, 'frequency': bigrams[item]}\n",
    "        result.append(dictionary)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_most_frequent_bigrams(data, k, collection_to_save):\n",
    "    bigram_dict   = count_bigrams(data)\n",
    "    saveable_dict = bigrams_to_dict(bigram_dict)\n",
    "    save_to_mongo(collection_to_save, saveable_dict, True)\n",
    "    most_frequent = get_k_most_frequent(bigram_dict, k)\n",
    "    pretty_print_most_frequent(most_frequent)\n",
    "    return most_frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13480027 rows are saved to \"bigrams\" collection successfully!\n",
      "Platz 1: Die Wortkombination \"e mail\" kommt in den Daten 15108 mal vor.\n",
      "Platz 2: Die Wortkombination \"millionen euro\" kommt in den Daten 11995 mal vor.\n",
      "Platz 3: Die Wortkombination \"milliarden euro\" kommt in den Daten 8993 mal vor.\n",
      "Platz 4: Die Wortkombination \"antwort schreiben\" kommt in den Daten 7553 mal vor.\n",
      "Platz 5: Die Wortkombination \"seit jahren\" kommt in den Daten 6488 mal vor.\n",
      "Platz 6: Die Wortkombination \"new york\" kommt in den Daten 5108 mal vor.\n",
      "Platz 7: Die Wortkombination \"us dollar\" kommt in den Daten 4957 mal vor.\n",
      "Platz 8: Die Wortkombination \"mail adresse\" kommt in den Daten 4883 mal vor.\n",
      "Platz 9: Die Wortkombination \"orf at\" kommt in den Daten 4797 mal vor.\n",
      "Platz 10: Die Wortkombination \"vergangenen jahr\" kommt in den Daten 4776 mal vor.\n",
      "Wall time: 37min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extracted_documents = get_column_of_collection(sentences_preprocessed, 'sentence')\n",
    "most_frequent = get_k_most_frequent_bigrams(extracted_documents, 10, bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDos: \n",
    "- Einlesen von csv ohne Konvertierung ✓\n",
    "- Einfügen von sentences_preprocessed in DB ohne Umweg über DataFrame und .csv ✓\n",
    "- Spaltenname ohne Umbenennung ✓\n",
    "- Daten für Analyse aus DB ziehen ✓\n",
    "- Das Ergebnis (komplettes Bigram-Dict?) in der DB speichern ✓\n",
    "- Weitere Analysen?\n",
    "- Satzzeichen bei Stopwörtern hizufügen ✓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
